{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the Confusion matrix results\n",
    "\n",
    "def conf_matrix(yTest, yPred): \n",
    "    '''\n",
    "    This function plots the confusion matrix showing exact values of TP,TN,FP,FN\n",
    "    '''\n",
    "    conf=confusion_matrix(yTest, yPred)\n",
    "    cmap=sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0)\n",
    "    conf_heat_map=sns.heatmap(conf,cmap=cmap,xticklabels=['Yes','No'],annot=True,fmt=\"d\")\n",
    "    return conf_heat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall, f1-score, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Precision, Recall, f1 score, Accuracy score\n",
    "\n",
    "def precision_recall_f1_accuracy(yTest, yPred): \n",
    "    '''\n",
    "    This function gives the exact values of precision, recall and accuracy of the model\n",
    "    Range for precision,recall,f1,accuracy : 0 to 1 (0 being the worst and 1 being best for predictive model)\n",
    "    '''\n",
    "    recall=recall_score(yTest, yPred)\n",
    "    precision=precision_score(yTest, yPred)\n",
    "    f1_score=f1_score(yTest, yPred)\n",
    "    accuracy=accuracy_score(yTest, yPred)\n",
    "    print('recall_score:',recall)\n",
    "    print('precision_score:',precision)\n",
    "    print('f1_score_score:',f1_score)\n",
    "    print('accuracy_score:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Precision Recall Curve\n",
    "\n",
    "def prec_rec_curve(yTest, predicted_probas): \n",
    "    '''\n",
    "    This function plots the precision recall curve.\n",
    "    '''\n",
    "    prec_rec=skplt.metrics.plot_precision_recall_curve(y_test, predicted_probas)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot ROC AUC for each predictive model\n",
    "\n",
    "def plot_roc(yTest, yPred):\n",
    "    '''\n",
    "    This function plots the ROC and gives the AUC.\n",
    "    Range for Area under the curve : 0 to 1 (0 being the worst and 1 being best for predictive model)\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(yTest, yPred)\n",
    "    roc_auc = roc_auc_score(yTest, yPred)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='random')\n",
    "    plt.plot([0,0,1,1],[0,1,1,1],'g-',label='perfect')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Gini coefficient from ROAUC\n",
    "\n",
    "\n",
    "def get_gini(yTest, yPred): \n",
    "    '''\n",
    "    This function produces the gini coefficient output of the model. \n",
    "    The 'auto' method calculates this metric by using the roc_auc_score\n",
    "    function from sklearn. \n",
    "    The Gini Coefficient is the summary statistic of the Cumulative Accuracy Profile (CAP) chart.\n",
    "    Gini is nothing but ratio between area between the ROC curve and the diagnol line & the area of the above triangle\n",
    "    Range: -1 to 1 (-1 being the worst predictive model and 1 being the best)\n",
    "    Gini above 60% is a good model\n",
    "    '''\n",
    "    roc_auc = roc_auc_score(yTest, yPred)\n",
    "    gini    = round((2 * roc_auc) - 1, 3)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain and Lift Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gain and Lift charts\n",
    "\n",
    "def cum_gain_lift_chart(yTest, predicted_probas): \n",
    "    '''\n",
    "    This function plots the cumulative gain chart.\n",
    "    Cumulative gain chart: It is the graph between Cumulative %responded and Cummulative %Population\n",
    "    '''\n",
    "    cum_gain=skplt.metrics.plot_cumulative_gain(y_test, predicted_probas)\n",
    "    lift=skplt.metrics.plot_lift_curve(y_test, predicted_probas)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lift chart (between actual vs predicted values)\n",
    "\n",
    "def plotLiftChart(actual, predicted):\n",
    "    '''\n",
    "    This function plots the lift chart between actual and predicted values\n",
    "    '''\n",
    "    df_dict = {'actual': list (actual), 'pred': list(predicted)}\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    pred_ranks = pd.qcut(df['pred'].rank(method='first'), 100, labels=False)\n",
    "    actual_ranks = pd.qcut(df['actual'].rank(method='first'), 100, labels=False)\n",
    "    pred_percentiles = df.groupby(pred_ranks).mean()\n",
    "    actual_percentiles = df.groupby(actual_ranks).mean()\n",
    "    plt.title('Lift Chart')\n",
    "    plt.plot(np.arange(.01, 1.01, .01), np.array(pred_percentiles['pred']),\n",
    "             color='darkorange', lw=2, label='Prediction')\n",
    "    plt.plot(np.arange(.01, 1.01, .01), np.array(pred_percentiles['actual']),\n",
    "             color='navy', lw=2, linestyle='--', label='Actual')\n",
    "    plt.ylabel('Target Percentile')\n",
    "    plt.xlabel('Population Percentile')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
